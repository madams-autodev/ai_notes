Let's dive into the history of
artificial intelligence and see how we got to this exciting point
in history that we are at today. So AI has gone through several
cycles where there have been both significant amounts of
investments and excitement, as well as large walls in these
investments as well as the excitement. So going from earlier algorithms and
the initial excitement, as the term artificial intelligence was officially
coined, and the first investments and promise began to be discussed
on a larger scale in the 1950s. We then in the 1960s and 1970s,
hail our first AI Winter, as the old failed to live up to
the promise of the initial expectations. Then in the 1980s, AI got its second boom
as businesses found it possible to utilize rule-based algorithms, called expert
systems to aid in decision-making. Along with that, many theoretical breakthroughs introduce
the promise behind neural networks. But then as promised,
we had another AI Winter. In the late 80s and into the 90s,
another failure to be able to leverage the promise of the expert
systems and neural networks, and practical applications led to
another decrease in investments. Then in the late 90s and early 2000s,
machine learning proved to be successful in many tasks, such as speech recognition,
Google's search algorithm and many others. And then finally, fast-forwarding to
today, there were major breakthroughs in deep learning proving to be able
to overcome many of the historical practical limitations of
the initial neural networks. And they began to outperform classical
machine learning techniques in many complex big data problems, such as image
classification and machine translation. So we see this up and down, and then
finally hear this excitement around deep learning as they able to accomplish
all of these tasks that were once set with such high bar when we first started
to think about artificial intelligence. So let's go all the way
back to the beginning. In 1950,
Alan Turing developed the Turing test to test a machine's ability to
exhibit intelligent behavior. Alan Turing's test has served
as a foundational threshold for artificial intelligence. The idea being, will a computer ever be able to
sufficiently imitate a human to the point where a suspicious judge cannot tell
the difference between human and machine? So then in 1956, Artificial Intelligence was accepted
as a field at the Dartmouth Conference. This Dartmouth Conference was where the
term Artificial Intelligence was actually first coined. With this conference, researchers
from various fields excitedly agreed that artificial intelligence, that is
the simulation of intelligent behavior in computers, was indeed achievable. In 1957, Frank Rosenblatt invented
the perceptron algorithm, which is the precursor to
our modern neural networks. And it generated a ton of excitement in
the field, as it showed how one can come up with a model that actually learned
from data fed into the machine. And then in 1959, Arthur Samuel published
an algorithm for checkers program, which was intelligently built according
to the limitations of computation at the time obviously, to learn from
the current state of the board and the positions at seen in the past
to come up with the next move. And Arthur Samuel is well regarded for
popularizing the term machine learning. So we see here, in the 50s, the advent
of these key terms that we've discussed artificial intelligence and
machine learning, as well as the precursor to deep learning in
Frank Rosenblatt's perceptron algorithm. As promised following this excitement,
we face our first AI Winter. During the Cold War, machine translation,
specifically between Russian and English, was one of the major
goals in artificial intelligence. In 1966, a committee was formed of seven
scientists, put in place by the US government, specifically to assess
the progress of machine translation. And this group deemed there was
very little return on investment, dealing a major blow to the hype
around artificial intelligence. Then in 1969, Marvin Minsky's
work identified major limitations to Rosenblatt's perceptron algorithm,
dealing yet another blow to artificial intelligence. Then in 1973, the highly regarded British
applied mathematician, James Lighthill published a report highlighting how
discoveries and research have fallen well short of the high hopes laid out over the
past few decades, leading to government's cutting back much of their funding
in artificial intelligence research. These major public reports
led to large cuts and government funding for AI research
leading to our first AI Winter. Then as discuss, we have our winter,
then we have our next boom. In the 1980s,
expert systems became all the rage. These systems were built with programmed
rules designed to mimic human experts, the idea being to hard-code
the machines with facts and rules representing expert
knowledge of different subjects. These programs ran on powerful mainframe
computers using specialized programming languages, such as LISP, which is one
of the oldest high-level languages and was the most popular for artificial
intelligence research at its time. These expert systems
gained huge popularity. And AI infiltrated the business
world at a large scale for the first time here in the 1980s. And then on another note,
another significant event in the history of AI during this time,
was Geoffrey Hinton and others published works explaining
the Backpropagation algorithm, possibly, the most important algorithm for
deep learning, which allowed for multi-layer networks to better update
its parameters using the data given. This drove a huge excitement in the fact
that researchers could now theoretically learn complex multi-layer models that
can actually learn from new data. And then we hit our next
AI Winter in the 80s and 90s. Expert systems' progress on
solving business problems slowed, major reasons being their inability
to learn, also they were finicky. They could make terrible mistakes
when they received abnormal inputs. Expert systems began to be
melded into software suites of general business applications that
could run on PCs instead of mainframes. So the investment in large
mainframe computer subsided, as much of their capabilities were
incorporated into these regular desktops. Neural networks didn't
scale to large problems. The Backpropagation algorithm that
generates so much excitement faced many problems with implementation of problems
with large data sets and large networks. And then with these in mind,
the excitement behind utilizing artificial intelligence for
business once again subsided. So we'll take a break here
before our next video. And in our next video, we'll discuss the successes as this is the
last AI Winter that led to present day.