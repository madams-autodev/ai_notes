# What is Tokenization?

* *Definition:* Tokenization is the process of breaking text into smaller parts called **tokens**.
* *Why:* Models can’t understand full sentences the way humans do, so we break them down into pieces the model can process.

Example:
  Sentence: “IBM taught me tokenization”
  Tokens: `["IBM", "taught", "me", "tokenization"]`


# Types of Tokenization

## 1. Word-based tokenization
* Splits text into whole words.
*Advantage:* Keeps meaning intact.
*Disadvantage:* Vocabulary becomes huge (`unicorn` and `unicorns` treated as different tokens).
*Example:* “I love unicorns”* → `["I", "love", "unicorns"]`

---

### 2. **Character-based tokenization**

* Splits text into characters.
* **Advantage:** Very small vocabulary.
* **Disadvantage:** Single letters don’t mean much → harder for the model to capture meaning.
* **Example:**
  *“cat”* → `["c", "a", "t"]`

---

### 3. **Subword-based tokenization**

* Breaks down rare words into smaller chunks but keeps common words intact.
* **Advantage:** Best of both worlds → handles new/rare words while keeping frequent words whole.
* **Example:**
  *“unhappiness”* → `["un", "happiness"]`
  *“cats”* → `["cat", "s"]`

---

## 🔧 Algorithms for Subword Tokenization

* **WordPiece** (used in BERT): Merges/splits tokens based on usefulness.
* **Unigram:** Starts with a big set of pieces → gradually removes the less useful ones.
* **SentencePiece:** Splits text into chunks and assigns each a unique ID (good for multiple languages).

---

## ⚙️ How Tokenization Works in Libraries

### PyTorch (`torchtext`)

* **Step 1:** Use `get_tokenizer()` to break sentences into tokens.
* **Step 2:** Build a **vocabulary** (list of all unique tokens).
* **Step 3:** Assign each token an **index (number)** → this is what the model understands.
* **Step 4:** Use special tokens:

  * **UNK** → unknown token (for words not in the vocabulary).
  * **BOS** → Beginning of Sentence.
  * **EOS** → End of Sentence.
  * **PAD** → for making all sentences the same length.

**Example:**
Sentence: *“I love AI”*
Tokens: `["I", "love", "AI"]`
Indices: `[2, 17, 54]`

---

## 📌 Recap

* **Tokenization = breaking text into tokens.**
* **Word-based** → whole words (large vocab).
* **Character-based** → single letters (small vocab, low meaning).
* **Subword-based** → keeps common words whole, splits rare ones (balanced).
* **Special tokens** help models know where sentences start/end and keep input lengths consistent.

---

👉 Would you like me to also create a **side-by-side comparison table** of the 3 tokenization types (word, char, subword) with pros, cons, and examples? That could make it crystal clear.





Got it 👍 Let’s explain **vocabulary** in tokenization like you’re 5.

---

# 📖 Vocabulary in Tokenization (Dummy-friendly)

### What is "Vocabulary"?

* In real life, **your vocabulary** = all the words you know.
* In AI/Tokenization, **the vocabulary** = all the tokens the model knows.

Think of it as the **dictionary of tokens** the AI can understand.

---

### Example 1: Word-based tokenization

If your model only saw these sentences:

* “I love cats”
* “Cats are cute”

Then the **vocabulary** would be:
`["I", "love", "cats", "are", "cute"]`

Each word gets a number (ID):

* I → 0
* love → 1
* cats → 2
* are → 3
* cute → 4

So the sentence *“I love cats”* becomes: `[0, 1, 2]`

---

### Example 2: Character-based tokenization

Same sentences, but split into characters. Vocabulary becomes:
`["I", "l", "o", "v", "e", "c", "a", "t", "s", "r", "u"]`

Much smaller vocab, but the model has to work harder to understand meaning.

---

### Example 3: Subword-based tokenization

Rare words are split. For example:

* “unhappiness” → `["un", "happy", "ness"]`

So the vocabulary stores **common chunks**, not just whole words or letters.

---

### Special tokens in Vocabulary

Models also add **extra tokens**:

* **\[UNK]** → unknown word (if the word isn’t in the vocab)
* **\[PAD]** → padding, to make all inputs the same length
* **\[BOS]** → beginning of sentence
* **\[EOS]** → end of sentence

---

✅ **In short:**

* Vocabulary = the list of all tokens a model knows.
* Each token gets a unique number (ID).
* Sentences are converted into these IDs before being fed to the model.

---

Would you like me to also give you a **visual diagram (boxes & arrows)** showing how a sentence turns into tokens, then into numbers, and finally into vocabulary IDs? That might make it super clear.

