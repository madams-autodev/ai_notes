# Activation Functions in Neural Networks

## 1. Why Do We Need Activation Functions?
- Neural networks combine inputs with weights & biases â†’ but without activation functions, itâ€™s just **linear math**.
- Activation functions make the network **non-linear**, which allows:
  - Detecting complex patterns (images, text, voice, etc.)
  - Enabling deep learning.

Think of them as **"decision gates"** that decide whether a neuron should "fire" or not.

---

## 2. ğŸ§® Common Activation Functions

### 2.1 ğŸŒ€ Sigmoid
Formula:  
\[
\sigma(z) = \frac{1}{1 + e^{-z}}
\]

- Outputs between **0 and 1**.
- Example:  
  - If input = 0 â†’ output = 0.5  
  - If input = 10 â†’ output â‰ˆ 1  
  - If input = -10 â†’ output â‰ˆ 0  

âœ… Good for probabilities.  
âŒ **Problems**:
- **Vanishing Gradient**: For very high or low inputs, slope â‰ˆ 0 â†’ network stops learning.  
- Outputs only positive (0â€“1), not centered around 0.

---

### 2.2 ğŸ“ˆ Hyperbolic Tangent (tanh)
Formula:  
\[
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
\]

- Outputs between **-1 and +1**.  
- Symmetric around 0 â†’ fixes the "always positive" issue of Sigmoid.  

âœ… Centered at 0 â†’ better than sigmoid.  
âŒ Still suffers from **vanishing gradient** in deep networks.  

---

### 2.3 ğŸ”² ReLU (Rectified Linear Unit)
Formula:  
\[
f(z) = \max(0, z)
\]

- If input < 0 â†’ output = 0  
- If input â‰¥ 0 â†’ output = input  

âœ… Advantages:
- Simple & fast.  
- Helps avoid vanishing gradient.  
- Sparse activation: only some neurons fire â†’ efficient.  

âŒ Problem: "Dead ReLU" â†’ some neurons may **always output 0** if they only get negative inputs.

---

### 2.4 âš¡ Leaky ReLU
Formula:  
\[
f(z) = \begin{cases} 
z & \text{if } z > 0 \\ 
0.01z & \text{if } z \leq 0 
\end{cases}
\]

- Fix for "Dead ReLU" â†’ gives small slope for negative values.  

---

### 2.5 ğŸ“Š Softmax
Formula:  
\[
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
\]

- Converts numbers â†’ **probability distribution**.  
- Example: raw output = `[1.6, 0.55, 0.98]`  
  - After softmax = `[0.51, 0.18, 0.31]`  

âœ… Used in **output layer for classification** (pick the class with highest probability).  

---

## 3. ğŸš€ Which Functions Are Used Today?
- **Sigmoid**: Rarely used (hidden layers), only for probabilities.  
- **Tanh**: Better than sigmoid, but still causes vanishing gradient.  
- **ReLU**: ğŸ”¥ Most popular for hidden layers.  
- **Leaky ReLU**: Used when ReLU has â€œdead neuronsâ€.  
- **Softmax**: Used in output layer for **multi-class classification**.  

---

## 4. ğŸ¯ Quick Summary
- Sigmoid â†’ [0,1], probability but causes vanishing gradients.  
- Tanh â†’ [-1,1], symmetric but still vanishes in deep nets.  
- ReLU â†’ most common, fast, avoids vanishing gradient.  
- Leaky ReLU â†’ fix for dead ReLUs.  
- Softmax â†’ used in final layer for classification â†’ outputs probabilities.

---

ğŸ’¡ **Analogy**:
- Sigmoid = dimmer switch (values squeezed between 0â€“1).  
- Tanh = balanced dimmer (can be negative or positive).  
- ReLU = light switch (off = 0, on = brightness level).  
- Softmax = voting system (turns outputs into % probabilities).
```

---

ğŸ‘‰ Do you want me to also **draw simple plots** (Sigmoid curve, ReLU shape, Softmax distribution) so you can visualize each functionâ€™s behavior?
